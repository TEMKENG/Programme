\babel@toc {ngerman}{}
\babel@toc {ngerman}{}
\contentsline {section}{\numberline {1}Abk\IeC {\"u}rzungsverzeichnis}{7}{section.1}
\contentsline {section}{\numberline {2}Einleitung}{8}{section.2}
\contentsline {subsection}{\numberline {2.1}Motivation}{8}{subsection.2.1}
\contentsline {subsection}{\numberline {2.2}Ziel der Arbeit.}{8}{subsection.2.2}
\contentsline {subsection}{\numberline {2.3}Aufbau der Arbeit.}{9}{subsection.2.3}
\contentsline {section}{\numberline {3}Grundlagen}{10}{section.3}
\contentsline {subsection}{\numberline {3.1}K\IeC {\"u}nstliche neuronale Netzwerke}{10}{subsection.3.1}
\contentsline {subsubsection}{\numberline {3.1.1}K\IeC {\"u}nstliches Neuron}{10}{subsubsection.3.1.1}
\contentsline {subsubsection}{\numberline {3.1.2}Aktivierungskarten(Feature-Maps)}{10}{subsubsection.3.1.2}
\contentsline {subsubsection}{\numberline {3.1.3}Filters}{11}{subsubsection.3.1.3}
\contentsline {subsection}{\numberline {3.2}Convolutional Neural Network}{11}{subsection.3.2}
\contentsline {subsubsection}{\numberline {3.2.1}Feedforward}{11}{subsubsection.3.2.1}
\contentsline {paragraph}{\numberline {3.2.1.1}Input Layer}{11}{paragraph.3.2.1.1}
\contentsline {paragraph}{\numberline {3.2.1.2}Faltungsschicht}{11}{paragraph.3.2.1.2}
\contentsline {subparagraph}{\nonumberline Die Anzahl und Gr\IeC {\"o}\IeC {\ss }e von Filtern.}{12}{section*.12}
\contentsline {subparagraph}{\nonumberline Die Schrittgr\IeC {\"o}\IeC {\ss }e}{12}{section*.14}
\contentsline {subparagraph}{\nonumberline \textit {Padding}.}{13}{section*.16}
\contentsline {paragraph}{\numberline {3.2.1.3}Aktivierungsfunktion}{14}{paragraph.3.2.1.3}
\contentsline {subparagraph}{\nonumberline Logistische Funktion}{15}{section*.20}
\contentsline {subparagraph}{\nonumberline Tangens Hyperbolicus}{16}{section*.22}
\contentsline {subparagraph}{\nonumberline Rectified Linear Unit}{16}{section*.24}
\contentsline {subparagraph}{\nonumberline Leaky ReLU Funktion}{17}{section*.26}
\contentsline {subparagraph}{\nonumberline Softmax}{17}{section*.28}
\contentsline {paragraph}{\numberline {3.2.1.4}Pooling Layer}{18}{paragraph.3.2.1.4}
\contentsline {paragraph}{\numberline {3.2.1.5}Multi-layer Perzeptron (Fully Connected Layer)}{18}{paragraph.3.2.1.5}
\contentsline {subsubsection}{\numberline {3.2.2}Backforward}{19}{subsubsection.3.2.2}
\contentsline {paragraph}{\numberline {3.2.2.1}Fehlerfunktion}{19}{paragraph.3.2.2.1}
\contentsline {paragraph}{\numberline {3.2.2.2}Gradient}{20}{paragraph.3.2.2.2}
\contentsline {paragraph}{\numberline {3.2.2.3}Lernrate}{20}{paragraph.3.2.2.3}
\contentsline {paragraph}{\numberline {3.2.2.4}Gradientenabstiegsverfahren}{21}{paragraph.3.2.2.4}
\contentsline {subparagraph}{\nonumberline Ablauf eines Gradientenverfahrens im \ac {DNN}.}{21}{section*.32}
\contentsline {subparagraph}{\nonumberline Variante des Gradientenverfahrens}{21}{section*.34}
\contentsline {subsection}{\numberline {3.3}Datens\IeC {\"a}tze und Bibliothek}{24}{subsection.3.3}
\contentsline {subsubsection}{\numberline {3.3.1}Datens\IeC {\"a}tze}{24}{subsubsection.3.3.1}
\contentsline {subsubsection}{\numberline {3.3.2}Bibliotheken}{24}{subsubsection.3.3.2}
\contentsline {section}{\numberline {4}Kompression von \ac {DNN}}{25}{section.4}
\contentsline {subsection}{\numberline {4.1}Beschneidung des Netzwerks(\textit {Pruning Network})}{25}{subsection.4.1}
\contentsline {subsection}{\numberline {4.2}Quantisierung von neuronalen Netzwerken}{29}{subsection.4.2}
\contentsline {subsubsection}{\numberline {4.2.1}Matrixfaktorisierung}{29}{subsubsection.4.2.1}
\contentsline {subsubsection}{\numberline {4.2.2}Quantisierung mit weniger Bits (Low-bit Quantization)}{30}{subsubsection.4.2.2}
\contentsline {subsection}{\numberline {4.3}Huffman Codierung}{32}{subsection.4.3}
\contentsline {section}{\numberline {5}Experiment}{32}{section.5}
\contentsline {subsection}{\numberline {5.1}Analyse der Ergebnisse mit Hilfe von Metriken}{32}{subsection.5.1}
\contentsline {subsection}{\numberline {5.2}Entwurf eines neuronalen Faltunsnetzwerkes: TemkiNet.}{33}{subsection.5.2}
\contentsline {subsubsection}{\numberline {5.2.1}Art der Faltungsschichten.}{34}{subsubsection.5.2.1}
\contentsline {paragraph}{\numberline {5.2.1.1}Standard Convolution}{34}{paragraph.5.2.1.1}
\contentsline {paragraph}{\numberline {5.2.1.2}Depthwise Convolution}{35}{paragraph.5.2.1.2}
\contentsline {paragraph}{\numberline {5.2.1.3}Pointwise Convolution}{36}{paragraph.5.2.1.3}
\contentsline {paragraph}{\numberline {5.2.1.4}Depthwise Separable Convolution}{36}{paragraph.5.2.1.4}
\contentsline {subsubsection}{\numberline {5.2.2}Faltende neuronale Netzwerke}{37}{subsubsection.5.2.2}
\contentsline {paragraph}{\numberline {5.2.2.1}AlexNet}{37}{paragraph.5.2.2.1}
\contentsline {paragraph}{\numberline {5.2.2.2}Xception}{39}{paragraph.5.2.2.2}
\contentsline {paragraph}{\numberline {5.2.2.3}MobileNet}{39}{paragraph.5.2.2.3}
\contentsline {paragraph}{\numberline {5.2.2.4}TemkiNet}{40}{paragraph.5.2.2.4}
\contentsline {subsubsection}{\numberline {5.2.3}Vergleich zwischen CNNs}{41}{subsubsection.5.2.3}
\contentsline {subsection}{\numberline {5.3}Methoden und Hyperparameter zur Verbesserung der Netzwerkleistung.}{41}{subsection.5.3}
\contentsline {subsubsection}{\numberline {5.3.1}Datenvermehrung (\textit {Data Augmentation}).}{41}{subsubsection.5.3.1}
\contentsline {subsubsection}{\numberline {5.3.2}Dropout}{44}{subsubsection.5.3.2}
\contentsline {subsubsection}{\numberline {5.3.3}Aktivierungsfunktion.}{46}{subsubsection.5.3.3}
\contentsline {subsubsection}{\numberline {5.3.4}Optimierer.}{46}{subsubsection.5.3.4}
\contentsline {subsubsection}{\numberline {5.3.5}Batch-Normalisierung(BN).}{48}{subsubsection.5.3.5}
\contentsline {subsubsection}{\numberline {5.3.6}Bildgr\IeC {\"o}\IeC {\ss }e.}{49}{subsubsection.5.3.6}
\contentsline {subsubsection}{\numberline {5.3.7}Anzahl der Aktivierungskarten pro Schicht:}{50}{subsubsection.5.3.7}
\contentsline {subsubsection}{\numberline {5.3.8}Qualit\IeC {\"a}t des Datensatzes}{51}{subsubsection.5.3.8}
\contentsline {subsubsection}{\numberline {5.3.9}Einfluss der Lernrate}{52}{subsubsection.5.3.9}
\contentsline {subsection}{\numberline {5.4}Verringerung des Speicherbedarfs}{54}{subsection.5.4}
\contentsline {subsubsection}{\numberline {5.4.1}Extreme Version von \textit {TemkiNet.}}{54}{subsubsection.5.4.1}
\contentsline {subsubsection}{\numberline {5.4.2}Pruning}{56}{subsubsection.5.4.2}
\contentsline {subsubsection}{\numberline {5.4.3}Quantisierung}{57}{subsubsection.5.4.3}
\contentsline {subsection}{\numberline {5.5}Transfer-Lernen (\textit {Transfer Learning})}{58}{subsection.5.5}
\contentsline {section}{\numberline {6}Zusammenfassung und Ausblick}{59}{section.6}
\contentsline {subsection}{\numberline {6.1}Zusammenfassung}{59}{subsection.6.1}
\contentsline {subsection}{\numberline {6.2}Ausblick}{60}{subsection.6.2}
