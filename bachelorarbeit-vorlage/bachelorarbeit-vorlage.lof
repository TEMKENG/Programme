\babel@toc {ngerman}{}
\babel@toc {ngerman}{}
\contentsline {figure}{\numberline {1}{\ignorespaces Funktionsweise eines k\IeC {\"u}nstlichen Neurons \relax }}{5}{figure.caption.6}
\contentsline {figure}{\numberline {2}{\ignorespaces Darstellung eines k\IeC {\"u}nstlichen neuronalen Netzes \relax }}{6}{figure.caption.7}
\contentsline {figure}{\numberline {3}{\ignorespaces Faltungsoperation \relax }}{6}{figure.caption.8}
\contentsline {figure}{\numberline {4}{\ignorespaces Bin\IeC {\"a}re Treppenfunktion\relax }}{7}{figure.caption.9}
\contentsline {figure}{\numberline {6}{\ignorespaces Lineare Funktion\relax }}{7}{figure.caption.10}
\contentsline {figure}{\numberline {8}{\ignorespaces Logistische Aktivierungsfunktion:$ sigmoid(x) $.\relax }}{8}{figure.caption.11}
\contentsline {figure}{\numberline {10}{\ignorespaces Tangens Hyperbolicus.\relax }}{8}{figure.caption.12}
\contentsline {figure}{\numberline {12}{\ignorespaces ReLU Aktivierungsfunktion\relax }}{9}{figure.caption.13}
\contentsline {figure}{\numberline {14}{\ignorespaces Leaky ReLU Funktion\relax }}{9}{figure.caption.14}
\contentsline {figure}{\numberline {16}{\ignorespaces Funktionsweise eines Max-Pooling-Layer\relax }}{10}{figure.caption.15}
\contentsline {figure}{\numberline {17}{\ignorespaces Funktionsweise eines Average-Pooling-Layer \relax }}{11}{figure.caption.16}
\contentsline {figure}{\numberline {18}{\ignorespaces Ablauf der Backpropagation\relax }}{15}{figure.caption.18}
\contentsline {figure}{\numberline {19}{\ignorespaces Ablauf der Netzbeschneidung (\textit {Pruning Network})\relax }}{20}{figure.caption.19}
\contentsline {figure}{\numberline {20}{\ignorespaces Anwendung von \textit {ImageDataAugmentation} \relax }}{23}{figure.caption.20}
\contentsline {figure}{\numberline {21}{\ignorespaces Neuronales Netz mit Dropout \cite {3}\relax }}{25}{figure.caption.22}
\contentsline {figure}{\numberline {22}{\ignorespaces \textbf {Links}: Ein Neuron zur Trainingszeit, die mit Wahrscheinlichkeit $ p $ vorhanden ist und mit Neuronen in der n\IeC {\"a}chsten Schicht mit Gewichten w verbunden ist. \textbf {Recht}: Zur Testzeit ist das Neuron immer vorhanden und die Gewichte werden mit $ p $ multipliziert. Die Ausgabe zur Testzeit ist identisch mit der erwarteten Ausgabe zur Trainingszeit\cite {3}.\relax }}{26}{figure.caption.23}
