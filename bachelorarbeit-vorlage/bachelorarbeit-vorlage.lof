\babel@toc {ngerman}{}
\babel@toc {ngerman}{}
\contentsline {figure}{\numberline {1}{\ignorespaces Funktionsweise eines k\IeC {\"u}nstlichen Neurons \relax }}{5}{figure.caption.6}
\contentsline {figure}{\numberline {2}{\ignorespaces Darstellung eines k\IeC {\"u}nstlichen neuronalen Netzes \relax }}{6}{figure.caption.7}
\contentsline {figure}{\numberline {3}{\ignorespaces Faltungsoperation mit einem $ 3\times 3 -$Filter und Schrittgr\IeC {\"o}\IeC {\ss }e $ =1 $\relax }}{7}{figure.caption.9}
\contentsline {figure}{\numberline {4}{\ignorespaces Faltungsoperation mit einem $ 3\times 3 -$Filter und Schrittgr\IeC {\"o}\IeC {\ss }e $ =2 $\relax }}{7}{figure.caption.10}
\contentsline {figure}{\numberline {5}{\ignorespaces Bin\IeC {\"a}re Treppenfunktion\relax }}{8}{figure.caption.11}
\contentsline {figure}{\numberline {7}{\ignorespaces Lineare Funktion\relax }}{9}{figure.caption.12}
\contentsline {figure}{\numberline {9}{\ignorespaces Logistische Aktivierungsfunktion:$ sigmoid(x) $.\relax }}{9}{figure.caption.13}
\contentsline {figure}{\numberline {11}{\ignorespaces Tangens Hyperbolicus.\relax }}{10}{figure.caption.14}
\contentsline {figure}{\numberline {13}{\ignorespaces ReLU Aktivierungsfunktion\relax }}{10}{figure.caption.15}
\contentsline {figure}{\numberline {15}{\ignorespaces Leaky ReLU Funktion\relax }}{10}{figure.caption.16}
\contentsline {figure}{\numberline {17}{\ignorespaces Funktionsweise eines Max-Pooling-Layer\relax }}{11}{figure.caption.17}
\contentsline {figure}{\numberline {18}{\ignorespaces Funktionsweise eines Average-Pooling-Layer \relax }}{12}{figure.caption.18}
\contentsline {figure}{\numberline {19}{\ignorespaces Ablauf der Backpropagation\relax }}{16}{figure.caption.20}
\contentsline {figure}{\numberline {20}{\ignorespaces Ablauf der Netzbeschneidung (\textit {Pruning Network})\relax }}{21}{figure.caption.21}
\contentsline {figure}{\numberline {21}{\ignorespaces Anwendung von \textit {ImageDataAugmentation} \relax }}{24}{figure.caption.22}
\contentsline {figure}{\numberline {22}{\ignorespaces Neuronales Netz mit Dropout \cite {3}\relax }}{26}{figure.caption.24}
\contentsline {figure}{\numberline {23}{\ignorespaces \textbf {Links}: Ein Neuron zur Trainingszeit, die mit Wahrscheinlichkeit $ p $ vorhanden ist und mit Neuronen in der n\IeC {\"a}chsten Schicht mit Gewichten w verbunden ist. \textbf {Recht}: Zur Testzeit ist das Neuron immer vorhanden und die Gewichte werden mit $ p $ multipliziert. Die Ausgabe zur Testzeit ist identisch mit der erwarteten Ausgabe zur Trainingszeit\cite {3}.\relax }}{27}{figure.caption.25}
