\babel@toc {ngerman}{}
\babel@toc {ngerman}{}
\contentsline {figure}{\numberline {1}{\ignorespaces Funktionsweise eines k\IeC {\"u}nstlichen Neurons \relax }}{5}{figure.caption.7}
\contentsline {figure}{\numberline {2}{\ignorespaces Faltungsoperation mit einem $ 3\times 3 -$Filter und Schrittgr\IeC {\"o}\IeC {\ss }e $ =1 $\relax }}{7}{figure.caption.8}
\contentsline {figure}{\numberline {3}{\ignorespaces Faltungsoperation mit einem $ 3\times 3 -$Filter und Schrittgr\IeC {\"o}\IeC {\ss }e $ =2 $\relax }}{7}{figure.caption.9}
\contentsline {figure}{\numberline {4}{\ignorespaces Bin\IeC {\"a}re Treppenfunktion\relax }}{8}{figure.caption.10}
\contentsline {figure}{\numberline {6}{\ignorespaces Lineare Funktion\relax }}{8}{figure.caption.11}
\contentsline {figure}{\numberline {8}{\ignorespaces Logistische Aktivierungsfunktion:$ sigmoid(x) $.\relax }}{9}{figure.caption.12}
\contentsline {figure}{\numberline {10}{\ignorespaces Tangens Hyperbolicus.\relax }}{9}{figure.caption.13}
\contentsline {figure}{\numberline {12}{\ignorespaces ReLU Aktivierungsfunktion\relax }}{9}{figure.caption.14}
\contentsline {figure}{\numberline {14}{\ignorespaces Leaky ReLU Funktion\relax }}{10}{figure.caption.15}
\contentsline {figure}{\numberline {16}{\ignorespaces Funktionsweise eines Max-Pooling-Layer\relax }}{11}{figure.caption.16}
\contentsline {figure}{\numberline {17}{\ignorespaces Funktionsweise eines Average-Pooling-Layer \relax }}{11}{figure.caption.19}
\contentsline {figure}{\numberline {18}{\ignorespaces Darstellung eines neuronalen Netzes \relax }}{12}{figure.caption.20}
\contentsline {figure}{\numberline {19}{\ignorespaces Ablauf der Backpropagation\relax }}{15}{figure.caption.21}
\contentsline {figure}{\numberline {20}{\ignorespaces AlexNet Architektur \href {https://neurohive.io/en/popular-networks/alexnet-imagenet-classification-with-deep-convolutional-neural-networks/}{Link zum Bild} \relax }}{18}{figure.caption.22}
\contentsline {figure}{\numberline {21}{\ignorespaces SqueezeNet Architektur \href {https://www.kdnuggets.com/2016/09/deep-learning-reading-group-squeezenet.html}{Link zum Bild} \relax }}{19}{figure.caption.23}
\contentsline {figure}{\numberline {22}{\ignorespaces fire\_module \href {https://www.kdnuggets.com/2016/09/deep-learning-reading-group-squeezenet.html}{Link zum Bild} \relax }}{20}{figure.caption.24}
\contentsline {figure}{\numberline {23}{\ignorespaces ff \relax }}{21}{figure.caption.25}
\contentsline {figure}{\numberline {24}{\ignorespaces Xception Architektur \relax }}{22}{figure.caption.26}
\contentsline {figure}{\numberline {25}{\ignorespaces Ablauf der Netzbeschneidung (\textit {Pruning Network})\relax }}{23}{figure.caption.27}
\contentsline {figure}{\numberline {26}{\ignorespaces Anwendung von \textit {ImageDataAugmentation} \relax }}{25}{figure.caption.28}
\contentsline {figure}{\numberline {27}{\ignorespaces Neuronales Netz mit Dropout \cite {3}\relax }}{27}{figure.caption.30}
\contentsline {figure}{\numberline {28}{\ignorespaces \textbf {Links}: Ein Neuron zur Trainingszeit, die mit Wahrscheinlichkeit $ p $ vorhanden ist und mit Neuronen in der n\IeC {\"a}chsten Schicht mit Gewichten w verbunden ist. \textbf {Recht}: Zur Testzeit ist das Neuron immer vorhanden und die Gewichte werden mit $ p $ multipliziert. Die Ausgabe zur Testzeit ist identisch mit der erwarteten Ausgabe zur Trainingszeit\cite {3}.\relax }}{28}{figure.caption.31}
