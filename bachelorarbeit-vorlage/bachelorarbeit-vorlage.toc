\babel@toc {ngerman}{}
\babel@toc {ngerman}{}
\contentsline {section}{\numberline {1}Einleitung}{4}{section.1}
\contentsline {section}{\numberline {2}Grundlagen}{5}{section.2}
\contentsline {subsection}{\numberline {2.1}Neuron}{5}{subsection.2.1}
\contentsline {subsection}{\numberline {2.2}Merkmalskarten(Feature-Maps)}{5}{subsection.2.2}
\contentsline {subsection}{\numberline {2.3}Filters}{6}{subsection.2.3}
\contentsline {subsection}{\numberline {2.4}Entwicklung von K\IeC {\"u}nstlichen Neuronalen Netzen}{6}{subsection.2.4}
\contentsline {section}{\numberline {3}Convolutional Neural Network}{6}{section.3}
\contentsline {subsection}{\numberline {3.1}Feedforward}{6}{subsection.3.1}
\contentsline {subsubsection}{\numberline {3.1.1}Input Layer}{6}{subsubsection.3.1.1}
\contentsline {subsubsection}{\numberline {3.1.2}Faltungsschicht}{6}{subsubsection.3.1.2}
\contentsline {subsubsection}{\numberline {3.1.3}Aktivierungsfunktion}{9}{subsubsection.3.1.3}
\contentsline {subsubsection}{\numberline {3.1.4}Pooling Layer}{12}{subsubsection.3.1.4}
\contentsline {subsubsection}{\numberline {3.1.5}Multi-layer Perzeptron (Fully Connected Layer)}{14}{subsubsection.3.1.5}
\contentsline {subsection}{\numberline {3.2}Backforward}{14}{subsection.3.2}
\contentsline {subsubsection}{\numberline {3.2.1}Fehlerfunktion}{14}{subsubsection.3.2.1}
\contentsline {subsubsection}{\numberline {3.2.2}Gradient}{15}{subsubsection.3.2.2}
\contentsline {subsubsection}{\numberline {3.2.3}Lernrate}{15}{subsubsection.3.2.3}
\contentsline {subsubsection}{\numberline {3.2.4}Gradientenabstiegsverfahren}{16}{subsubsection.3.2.4}
\contentsline {paragraph}{\numberline {3.2.4.1}Ablauf eines Gradientenverfahrens im \ac {DNN}}{16}{paragraph.3.2.4.1}
\contentsline {paragraph}{\numberline {3.2.4.2}Variante des Gradientenverfahrens}{16}{paragraph.3.2.4.2}
\contentsline {subparagraph}{\nonumberline Stochastic Gradient Descent (SGD):}{17}{section*.25}
\contentsline {subparagraph}{\nonumberline Batch Gradient Descent (BGD):}{17}{section*.26}
\contentsline {subparagraph}{\nonumberline Mini-batch Stochastic Gradient Descent(MSGD):}{18}{section*.27}
\contentsline {section}{\numberline {4}Kompression von \ac {DNN}}{18}{section.4}
\contentsline {subsection}{\numberline {4.1}Pruning Network}{19}{subsection.4.1}
\contentsline {subsection}{\numberline {4.2}Quantisierung von \ac {NN}}{19}{subsection.4.2}
\contentsline {subsection}{\numberline {4.3}Huffman Codierung}{21}{subsection.4.3}
\contentsline {section}{\numberline {5}Overfitting in Convolutional neuronale Netzwerke}{21}{section.5}
\contentsline {subsection}{\numberline {5.1}Overfitting Definition}{21}{subsection.5.1}
\contentsline {subsection}{\numberline {5.2}Strategie gegen Overfitting}{22}{subsection.5.2}
\contentsline {subsubsection}{\numberline {5.2.1}Data Augmentation}{22}{subsubsection.5.2.1}
\contentsline {subsubsection}{\numberline {5.2.2}Dropout}{23}{subsubsection.5.2.2}
\contentsline {paragraph}{\numberline {5.2.2.1}Funktionsweise von Dropout}{24}{paragraph.5.2.2.1}
\contentsline {paragraph}{\numberline {5.2.2.2}Verhinderung der Koadaptationen zwischen Neuronen}{24}{paragraph.5.2.2.2}
\contentsline {paragraph}{\numberline {5.2.2.3}Automatische Erh\IeC {\"o}hung von Training Data und Regelung}{25}{paragraph.5.2.2.3}
\contentsline {subsubsection}{\numberline {5.2.3}Batch-Normalisierung}{25}{subsubsection.5.2.3}
\contentsline {section}{\numberline {6}Experiment}{27}{section.6}
\contentsline {subsection}{\numberline {6.1}Besondere \acsp {CNN}}{27}{subsection.6.1}
\contentsline {subsubsection}{\numberline {6.1.1}AlexNet}{27}{subsubsection.6.1.1}
\contentsline {subsubsection}{\numberline {6.1.2}SqueezeNet}{28}{subsubsection.6.1.2}
\contentsline {subsubsection}{\numberline {6.1.3}Xception: Extreme Inception}{29}{subsubsection.6.1.3}
\contentsline {subsubsection}{\numberline {6.1.4}MobileNet}{30}{subsubsection.6.1.4}
\contentsline {subsection}{\numberline {6.2}Lernrate}{30}{subsection.6.2}
\contentsline {subsection}{\numberline {6.3}Algorithmen zur Optimierung des Gradientenabstiegsverfahren: Optimizer}{32}{subsection.6.3}
\contentsline {subsubsection}{\numberline {6.3.1}Adaptive Gradient Algorithm (AdaGrad)}{32}{subsubsection.6.3.1}
\contentsline {subsubsection}{\numberline {6.3.2}Root Mean Square Propagation(RMSProp)}{32}{subsubsection.6.3.2}
\contentsline {subsubsection}{\numberline {6.3.3}Adaptive Moment Estimation(Adam)}{33}{subsubsection.6.3.3}
\contentsline {section}{\numberline {7}Abk\IeC {\"u}rzungsverzeichnis}{33}{section.7}
