\babel@toc {ngerman}{}
\babel@toc {ngerman}{}
\contentsline {section}{\numberline {1}Einleitung}{4}{section.1}
\contentsline {section}{\numberline {2}Grundlagen}{5}{section.2}
\contentsline {subsection}{\numberline {2.1}Entwicklung von K\IeC {\"u}nstlichen Neuronalen Netzen}{5}{subsection.2.1}
\contentsline {section}{\numberline {3}Feedforward}{5}{section.3}
\contentsline {section}{\numberline {4}Layer in \ac {CNN}}{5}{section.4}
\contentsline {subsection}{\numberline {4.1}Input Layer}{5}{subsection.4.1}
\contentsline {subsection}{\numberline {4.2}Faltungsschicht(\textit {Convolution Layer})}{6}{subsection.4.2}
\contentsline {subsection}{\numberline {4.3}Aktivierungsfunktion}{7}{subsection.4.3}
\contentsline {subsection}{\numberline {4.4}Pooling Layer}{10}{subsection.4.4}
\contentsline {subsection}{\numberline {4.5}Multi-layer Perzeptron (Fully Connected Layer)}{11}{subsection.4.5}
\contentsline {section}{\numberline {5}Backforward}{12}{section.5}
\contentsline {subsection}{\numberline {5.1}Fehlerfunktion}{12}{subsection.5.1}
\contentsline {subsection}{\numberline {5.2}Gradientenabstieg}{13}{subsection.5.2}
\contentsline {subsection}{\numberline {5.3}Backpropagation: Optimizer}{13}{subsection.5.3}
\contentsline {paragraph}{\numberline {5.3.0.1}Warum wird Gradientenverfahren gebraucht}{13}{paragraph.5.3.0.1}
\contentsline {paragraph}{\numberline {5.3.0.2}Funktionieren von Gradientenverfahren}{14}{paragraph.5.3.0.2}
\contentsline {paragraph}{\numberline {5.3.0.3}Type von Gradientenverfahren}{14}{paragraph.5.3.0.3}
\contentsline {subparagraph}{\numberline {5.3.0.3.1}\textbf {Stochastic Gradient Descent:}\textit {SGD}}{14}{subparagraph.5.3.0.3.1}
\contentsline {subparagraph}{\numberline {5.3.0.3.2}\textbf {Batch Gradient Descent:}\textit {BGD}}{15}{subparagraph.5.3.0.3.2}
\contentsline {subparagraph}{\numberline {5.3.0.3.3}\textbf {Mini-batch Stochastic Gradient Descent:}\textit {MSGD}}{15}{subparagraph.5.3.0.3.3}
\contentsline {subsubsection}{\numberline {5.3.1}Adaptive Gradient Algorithm :\textit {AdaGrad}}{16}{subsubsection.5.3.1}
\contentsline {subsubsection}{\numberline {5.3.2}\textit {Root Mean Square Propagation:}RMSProp}{16}{subsubsection.5.3.2}
\contentsline {subsubsection}{\numberline {5.3.3}Adam \textit {Adaptive Moment Estimation:Adam}}{17}{subsubsection.5.3.3}
\contentsline {section}{\numberline {6}Besondere \acsp {CNN}}{17}{section.6}
\contentsline {subsection}{\numberline {6.1}AlexNet}{17}{subsection.6.1}
\contentsline {subsection}{\numberline {6.2}SqueezeNet}{19}{subsection.6.2}
\contentsline {subsection}{\numberline {6.3}Xception}{19}{subsection.6.3}
\contentsline {subsection}{\numberline {6.4}MobileNet}{19}{subsection.6.4}
\contentsline {section}{\numberline {7}Effizienter Nutzung tiefer neuronaler Netze}{19}{section.7}
\contentsline {subsection}{\numberline {7.1}Pruning Network}{20}{subsection.7.1}
\contentsline {subsection}{\numberline {7.2}Quantisierung von \ac {NN}}{22}{subsection.7.2}
\contentsline {subsection}{\numberline {7.3}Huffman Codierung}{24}{subsection.7.3}
\contentsline {section}{\numberline {8}Overfitting in Convolutional neuronale Netzwerke}{24}{section.8}
\contentsline {subsection}{\numberline {8.1}Overfitting Definition}{24}{subsection.8.1}
\contentsline {subsection}{\numberline {8.2}Strategie gegen Overfitting}{25}{subsection.8.2}
\contentsline {subsubsection}{\numberline {8.2.1}Data Augmentation}{25}{subsubsection.8.2.1}
\contentsline {subsubsection}{\numberline {8.2.2}Dropout}{26}{subsubsection.8.2.2}
\contentsline {paragraph}{\numberline {8.2.2.1}Funktionsweise von Dropout}{26}{paragraph.8.2.2.1}
\contentsline {paragraph}{\numberline {8.2.2.2}Verhinderung der Koadaptationen zwischen Neuronen}{27}{paragraph.8.2.2.2}
\contentsline {paragraph}{\numberline {8.2.2.3}Automatische Erh\IeC {\"o}hung von Training Data und Regelung}{27}{paragraph.8.2.2.3}
\contentsline {subsubsection}{\numberline {8.2.3}Batch-Normalisierung}{28}{subsubsection.8.2.3}
\contentsline {section}{\numberline {9}Experiment}{29}{section.9}
\contentsline {section}{\numberline {10}Abk\IeC {\"u}rzungsverzeichnis}{29}{section.10}
