\babel@toc {ngerman}{}
\babel@toc {ngerman}{}
\contentsline {section}{\numberline {1}Einleitung}{4}{section.1}
\contentsline {section}{\numberline {2}Grundlagen}{5}{section.2}
\contentsline {subsection}{\numberline {2.1}Neuron}{5}{subsection.2.1}
\contentsline {subsection}{\numberline {2.2}Feature-Maps}{5}{subsection.2.2}
\contentsline {subsection}{\numberline {2.3}Entwicklung von K\IeC {\"u}nstlichen Neuronalen Netzen}{5}{subsection.2.3}
\contentsline {section}{\numberline {3}\ac {CNN}}{5}{section.3}
\contentsline {subsection}{\numberline {3.1}Feedforward}{5}{subsection.3.1}
\contentsline {subsubsection}{\numberline {3.1.1}Input Layer}{5}{subsubsection.3.1.1}
\contentsline {subsubsection}{\numberline {3.1.2}Faltungsschicht(\textit {Convolution Layer})}{6}{subsubsection.3.1.2}
\contentsline {subsubsection}{\numberline {3.1.3}Aktivierungsfunktion}{7}{subsubsection.3.1.3}
\contentsline {subsubsection}{\numberline {3.1.4}Pooling Layer}{10}{subsubsection.3.1.4}
\contentsline {subsubsection}{\numberline {3.1.5}Multi-layer Perzeptron (Fully Connected Layer)}{11}{subsubsection.3.1.5}
\contentsline {subsection}{\numberline {3.2}Backforward}{12}{subsection.3.2}
\contentsline {subsubsection}{\numberline {3.2.1}Fehlerfunktion}{12}{subsubsection.3.2.1}
\contentsline {subsubsection}{\numberline {3.2.2}Gradient}{12}{subsubsection.3.2.2}
\contentsline {subsubsection}{\numberline {3.2.3}Lernrate}{13}{subsubsection.3.2.3}
\contentsline {subsubsection}{\numberline {3.2.4}Gradientenabstiegsverfahren}{13}{subsubsection.3.2.4}
\contentsline {paragraph}{\numberline {3.2.4.1}Ablauf eines Gradientenverfahrens im \ac {DNN}}{13}{paragraph.3.2.4.1}
\contentsline {paragraph}{\numberline {3.2.4.2}Variante des Gradientenverfahrens}{14}{paragraph.3.2.4.2}
\contentsline {subparagraph}{\nonumberline Stochastic Gradient Descent (SGD):}{15}{section*.23}
\contentsline {subparagraph}{\nonumberline Batch Gradient Descent (BGD):}{15}{section*.24}
\contentsline {subparagraph}{\nonumberline Mini-batch Stochastic Gradient Descent(MSGD):}{15}{section*.25}
\contentsline {section}{\numberline {4}Kompression von \ac {DNN}}{15}{section.4}
\contentsline {subsection}{\numberline {4.1}Pruning Network}{16}{subsection.4.1}
\contentsline {subsection}{\numberline {4.2}Quantisierung von \ac {NN}}{17}{subsection.4.2}
\contentsline {subsection}{\numberline {4.3}Huffman Codierung}{19}{subsection.4.3}
\contentsline {section}{\numberline {5}Overfitting in Convolutional neuronale Netzwerke}{19}{section.5}
\contentsline {subsection}{\numberline {5.1}Overfitting Definition}{19}{subsection.5.1}
\contentsline {subsection}{\numberline {5.2}Strategie gegen Overfitting}{19}{subsection.5.2}
\contentsline {subsubsection}{\numberline {5.2.1}Data Augmentation}{19}{subsubsection.5.2.1}
\contentsline {subsubsection}{\numberline {5.2.2}Dropout}{21}{subsubsection.5.2.2}
\contentsline {paragraph}{\numberline {5.2.2.1}Funktionsweise von Dropout}{21}{paragraph.5.2.2.1}
\contentsline {paragraph}{\numberline {5.2.2.2}Verhinderung der Koadaptationen zwischen Neuronen}{21}{paragraph.5.2.2.2}
\contentsline {paragraph}{\numberline {5.2.2.3}Automatische Erh\IeC {\"o}hung von Training Data und Regelung}{22}{paragraph.5.2.2.3}
\contentsline {subsubsection}{\numberline {5.2.3}Batch-Normalisierung}{22}{subsubsection.5.2.3}
\contentsline {section}{\numberline {6}Experiment}{24}{section.6}
\contentsline {subsection}{\numberline {6.1}Besondere \acsp {CNN}}{24}{subsection.6.1}
\contentsline {subsubsection}{\numberline {6.1.1}AlexNet}{24}{subsubsection.6.1.1}
\contentsline {subsubsection}{\numberline {6.1.2}SqueezeNet}{25}{subsubsection.6.1.2}
\contentsline {subsubsection}{\numberline {6.1.3}Xception}{27}{subsubsection.6.1.3}
\contentsline {subsubsection}{\numberline {6.1.4}MobileNet}{27}{subsubsection.6.1.4}
\contentsline {subsection}{\numberline {6.2}Lernrate}{27}{subsection.6.2}
\contentsline {subsection}{\numberline {6.3}Algorithmen zur Optimierung des Gradientenabstiegsverfahren: Optimizer}{28}{subsection.6.3}
\contentsline {subsubsection}{\numberline {6.3.1}Adaptive Gradient Algorithm (AdaGrad)}{28}{subsubsection.6.3.1}
\contentsline {subsubsection}{\numberline {6.3.2}Root Mean Square Propagation(RMSProp)}{29}{subsubsection.6.3.2}
\contentsline {subsubsection}{\numberline {6.3.3}Adaptive Moment Estimation(Adam)}{30}{subsubsection.6.3.3}
\contentsline {section}{\numberline {7}Abk\IeC {\"u}rzungsverzeichnis}{30}{section.7}
