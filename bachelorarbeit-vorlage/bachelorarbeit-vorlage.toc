\babel@toc {ngerman}{}
\babel@toc {ngerman}{}
\contentsline {section}{\numberline {1}Einleitung}{6}{section.1}
\contentsline {section}{\numberline {2}Grundlagen}{7}{section.2}
\contentsline {subsection}{\numberline {2.1}K\IeC {\"u}nstliche neuronale Netze}{7}{subsection.2.1}
\contentsline {subsubsection}{\numberline {2.1.1}Neuron}{7}{subsubsection.2.1.1}
\contentsline {subsubsection}{\numberline {2.1.2}Merkmalskarten(Feature-Maps)}{7}{subsubsection.2.1.2}
\contentsline {subsubsection}{\numberline {2.1.3}Filters}{8}{subsubsection.2.1.3}
\contentsline {subsubsection}{\numberline {2.1.4}Entwicklung von K\IeC {\"u}nstlichen Neuronalen Netzen}{8}{subsubsection.2.1.4}
\contentsline {subsection}{\numberline {2.2}Convolutional Neural Network}{8}{subsection.2.2}
\contentsline {subsubsection}{\numberline {2.2.1}Feedforward}{8}{subsubsection.2.2.1}
\contentsline {paragraph}{\numberline {2.2.1.1}Input Layer}{8}{paragraph.2.2.1.1}
\contentsline {paragraph}{\numberline {2.2.1.2}Faltungsschicht}{8}{paragraph.2.2.1.2}
\contentsline {paragraph}{\numberline {2.2.1.3}Aktivierungsfunktion}{11}{paragraph.2.2.1.3}
\contentsline {paragraph}{\numberline {2.2.1.4}Pooling Layer}{14}{paragraph.2.2.1.4}
\contentsline {paragraph}{\numberline {2.2.1.5}Multi-layer Perzeptron (Fully Connected Layer)}{15}{paragraph.2.2.1.5}
\contentsline {subsubsection}{\numberline {2.2.2}Backforward}{15}{subsubsection.2.2.2}
\contentsline {paragraph}{\numberline {2.2.2.1}Fehlerfunktion}{15}{paragraph.2.2.2.1}
\contentsline {paragraph}{\numberline {2.2.2.2}Gradient}{17}{paragraph.2.2.2.2}
\contentsline {paragraph}{\numberline {2.2.2.3}Lernrate}{17}{paragraph.2.2.2.3}
\contentsline {subparagraph}{\nonumberline Stochastic Gradient Descent (SGD)}{18}{section*.23}
\contentsline {subparagraph}{\nonumberline Batch Gradient Descent (BGD)}{19}{section*.25}
\contentsline {subparagraph}{\nonumberline Mini-batch Stochastic Gradient Descent(MSGD):}{20}{section*.27}
\contentsline {subsection}{\numberline {2.3}Datens\IeC {\"a}tze und Bibliothek}{20}{subsection.2.3}
\contentsline {subsubsection}{\numberline {2.3.1}Datens\IeC {\"a}tze}{20}{subsubsection.2.3.1}
\contentsline {subsubsection}{\numberline {2.3.2}Bibliotheken}{21}{subsubsection.2.3.2}
\contentsline {section}{\numberline {3}Kompression von \ac {DNN}}{21}{section.3}
\contentsline {subsection}{\numberline {3.1}Pruning Network}{22}{subsection.3.1}
\contentsline {subsection}{\numberline {3.2}Quantisierung von neuronalen Netzwerken}{26}{subsection.3.2}
\contentsline {subsection}{\numberline {3.3}Huffman Codierung}{28}{subsection.3.3}
\contentsline {section}{\numberline {4}Experiment}{28}{section.4}
\contentsline {subsection}{\numberline {4.1}Analyse der Ergebnisse mit Hilfe von Metriken}{28}{subsection.4.1}
\contentsline {subsection}{\numberline {4.2}Einfluss der Lernrate}{30}{subsection.4.2}
\contentsline {subsection}{\numberline {4.3}Convolution Layers}{30}{subsection.4.3}
\contentsline {subparagraph}{\nonumberline Standard Convolution}{31}{section*.36}
\contentsline {subparagraph}{\nonumberline Depthwise Convolution}{32}{section*.38}
\contentsline {subparagraph}{\nonumberline Pointwise Convolution}{33}{section*.40}
\contentsline {subparagraph}{\nonumberline Depthwise Separable Convolution}{34}{section*.41}
\contentsline {subsection}{\numberline {4.4}Besondere Konvolution Neuronale Netzwerke}{34}{subsection.4.4}
\contentsline {subsubsection}{\numberline {4.4.1}AlexNet}{34}{subsubsection.4.4.1}
\contentsline {subsubsection}{\numberline {4.4.2}SqueezeNet}{35}{subsubsection.4.4.2}
\contentsline {subsubsection}{\numberline {4.4.3}Xception}{37}{subsubsection.4.4.3}
\contentsline {subsubsection}{\numberline {4.4.4}MobileNet}{37}{subsubsection.4.4.4}
\contentsline {subsection}{\numberline {4.5}Algorithmen zur Optimierung des Gradientenabstiegsverfahren: Optimizer}{38}{subsection.4.5}
\contentsline {subsubsection}{\numberline {4.5.1}Adaptive Gradient Algorithm (AdaGrad)}{38}{subsubsection.4.5.1}
\contentsline {subsubsection}{\numberline {4.5.2}Root Mean Square Propagation(RMSProp)}{39}{subsubsection.4.5.2}
\contentsline {subsubsection}{\numberline {4.5.3}Adaptive Moment Estimation(Adam)}{40}{subsubsection.4.5.3}
\contentsline {subsection}{\numberline {4.6}Problem beim Training von Convolutional neuronale Netzwerke}{40}{subsection.4.6}
\contentsline {subsubsection}{\numberline {4.6.1}Overfitting}{40}{subsubsection.4.6.1}
\contentsline {subparagraph}{\nonumberline Data Augmentation}{41}{section*.47}
\contentsline {subparagraph}{\nonumberline Dropout}{42}{section*.50}
\contentsline {subparagraph}{\nonumberline Funktionsweise von Dropout}{43}{section*.52}
\contentsline {subparagraph}{\nonumberline Verhinderung der Koadaptationen zwischen Neuronen}{43}{section*.54}
\contentsline {subparagraph}{\nonumberline Automatische Erh\IeC {\"o}hung von Training Daten und Regelung}{44}{section*.55}
\contentsline {subparagraph}{\nonumberline Batch-Normalisierung}{44}{section*.57}
\contentsline {subsubsection}{\numberline {4.6.2}Dataset}{46}{subsubsection.4.6.2}
\contentsline {section}{\numberline {5}Literatur}{46}{section.5}
