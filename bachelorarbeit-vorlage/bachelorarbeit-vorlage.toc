\babel@toc {ngerman}{}
\babel@toc {ngerman}{}
\contentsline {section}{\numberline {1}Einleitung}{6}{section.1}
\contentsline {section}{\numberline {2}Grundlagen}{7}{section.2}
\contentsline {subsection}{\numberline {2.1}K\IeC {\"u}nstliche neuronale Netzwerke}{7}{subsection.2.1}
\contentsline {subsubsection}{\numberline {2.1.1}K\IeC {\"u}nstliches Neuron}{7}{subsubsection.2.1.1}
\contentsline {subsubsection}{\numberline {2.1.2}Merkmalskarten(Feature-Maps)}{7}{subsubsection.2.1.2}
\contentsline {subsubsection}{\numberline {2.1.3}Filters}{8}{subsubsection.2.1.3}
\contentsline {subsubsection}{\numberline {2.1.4}Entwicklung von K\IeC {\"u}nstlichen Neuronalen Netzen}{8}{subsubsection.2.1.4}
\contentsline {subsection}{\numberline {2.2}Convolutional Neural Network}{8}{subsection.2.2}
\contentsline {subsubsection}{\numberline {2.2.1}Feedforward}{8}{subsubsection.2.2.1}
\contentsline {paragraph}{\numberline {2.2.1.1}Input Layer}{8}{paragraph.2.2.1.1}
\contentsline {paragraph}{\numberline {2.2.1.2}Faltungsschicht}{9}{paragraph.2.2.1.2}
\contentsline {paragraph}{\numberline {2.2.1.3}Aktivierungsfunktion}{11}{paragraph.2.2.1.3}
\contentsline {paragraph}{\numberline {2.2.1.4}Pooling Layer}{14}{paragraph.2.2.1.4}
\contentsline {paragraph}{\numberline {2.2.1.5}Multi-layer Perzeptron (Fully Connected Layer)}{15}{paragraph.2.2.1.5}
\contentsline {subsubsection}{\numberline {2.2.2}Backforward}{16}{subsubsection.2.2.2}
\contentsline {paragraph}{\numberline {2.2.2.1}Fehlerfunktion}{16}{paragraph.2.2.2.1}
\contentsline {paragraph}{\numberline {2.2.2.2}Gradient}{17}{paragraph.2.2.2.2}
\contentsline {paragraph}{\numberline {2.2.2.3}Lernrate}{17}{paragraph.2.2.2.3}
\contentsline {subparagraph}{\nonumberline Stochastic Gradient Descent (SGD)}{18}{section*.23}
\contentsline {subparagraph}{\nonumberline Batch Gradient Descent (BGD)}{19}{section*.25}
\contentsline {subparagraph}{\nonumberline Mini-batch Stochastic Gradient Descent(MSGD):}{20}{section*.27}
\contentsline {subsection}{\numberline {2.3}Datens\IeC {\"a}tze und Bibliothek}{20}{subsection.2.3}
\contentsline {subsubsection}{\numberline {2.3.1}Datens\IeC {\"a}tze}{20}{subsubsection.2.3.1}
\contentsline {subsubsection}{\numberline {2.3.2}Bibliotheken}{21}{subsubsection.2.3.2}
\contentsline {section}{\numberline {3}Kompression von \ac {DNN}}{21}{section.3}
\contentsline {subsection}{\numberline {3.1}Pruning Network}{22}{subsection.3.1}
\contentsline {subsection}{\numberline {3.2}Quantisierung von neuronalen Netzwerken}{26}{subsection.3.2}
\contentsline {subsection}{\numberline {3.3}Huffman Codierung}{28}{subsection.3.3}
\contentsline {section}{\numberline {4}Experiment}{28}{section.4}
\contentsline {subsection}{\numberline {4.1}Analyse der Ergebnisse mit Hilfe von Metriken}{28}{subsection.4.1}
\contentsline {subsection}{\numberline {4.2}Convolution Layers}{30}{subsection.4.2}
\contentsline {subsubsection}{\numberline {4.2.1}Standard Convolution}{30}{subsubsection.4.2.1}
\contentsline {subsubsection}{\numberline {4.2.2}Depthwise Convolution}{31}{subsubsection.4.2.2}
\contentsline {subsubsection}{\numberline {4.2.3}Pointwise Convolution}{32}{subsubsection.4.2.3}
\contentsline {subsubsection}{\numberline {4.2.4}Depthwise Separable Convolution}{33}{subsubsection.4.2.4}
\contentsline {subsection}{\numberline {4.3}Vergleich zwischen Konvolution Neuronale Netzwerke}{33}{subsection.4.3}
\contentsline {subsubsection}{\numberline {4.3.1}AlexNet}{33}{subsubsection.4.3.1}
\contentsline {subsubsection}{\numberline {4.3.2}Xception}{34}{subsubsection.4.3.2}
\contentsline {subsubsection}{\numberline {4.3.3}MobileNet}{35}{subsubsection.4.3.3}
\contentsline {subsubsection}{\numberline {4.3.4}TemkiNet}{35}{subsubsection.4.3.4}
\contentsline {subsection}{\numberline {4.4}Verbesserung der Leistung eines Convolution Neuronalen Netzwerks}{36}{subsection.4.4}
\contentsline {subsection}{\numberline {4.5}Einfluss der Lernrate}{38}{subsection.4.5}
\contentsline {subsection}{\numberline {4.6}Algorithmen zur Optimierung des Gradientenabstiegsverfahren: Optimierer}{40}{subsection.4.6}
\contentsline {subsubsection}{\numberline {4.6.1}Adaptive Gradient Algorithm (AdaGrad)}{40}{subsubsection.4.6.1}
\contentsline {subsubsection}{\numberline {4.6.2}Root Mean Square Propagation(RMSProp)}{40}{subsubsection.4.6.2}
\contentsline {subsubsection}{\numberline {4.6.3}Adaptive Moment Estimation(Adam)}{41}{subsubsection.4.6.3}
\contentsline {subsection}{\numberline {4.7}Problem beim Training von Convolutional neuronale Netzwerke}{41}{subsection.4.7}
\contentsline {subsubsection}{\numberline {4.7.1}Overfitting}{41}{subsubsection.4.7.1}
\contentsline {subparagraph}{\nonumberline Data Augmentation}{42}{section*.43}
\contentsline {subparagraph}{\nonumberline Dropout}{44}{section*.46}
\contentsline {subparagraph}{\nonumberline Batch-Normalisierung}{45}{section*.50}
\contentsline {subsubsection}{\numberline {4.7.2}Datensatz}{47}{subsubsection.4.7.2}
