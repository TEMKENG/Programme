\babel@toc {ngerman}{}
\babel@toc {ngerman}{}
\contentsline {section}{\numberline {1}Einleitung}{4}{section.1}
\contentsline {section}{\numberline {2}Grundlagen}{5}{section.2}
\contentsline {subsection}{\numberline {2.1}Neuron}{5}{subsection.2.1}
\contentsline {subsection}{\numberline {2.2}Merkmalskarten(Feature-Maps)}{5}{subsection.2.2}
\contentsline {subsection}{\numberline {2.3}Filters}{6}{subsection.2.3}
\contentsline {subsection}{\numberline {2.4}Entwicklung von K\IeC {\"u}nstlichen Neuronalen Netzen}{6}{subsection.2.4}
\contentsline {section}{\numberline {3}Convolutional Neural Network}{6}{section.3}
\contentsline {subsection}{\numberline {3.1}Feedforward}{6}{subsection.3.1}
\contentsline {subsubsection}{\numberline {3.1.1}Input Layer}{6}{subsubsection.3.1.1}
\contentsline {subsubsection}{\numberline {3.1.2}Faltungsschicht}{6}{subsubsection.3.1.2}
\contentsline {subsubsection}{\numberline {3.1.3}Aktivierungsfunktion}{8}{subsubsection.3.1.3}
\contentsline {subsubsection}{\numberline {3.1.4}Pooling Layer}{11}{subsubsection.3.1.4}
\contentsline {subsubsection}{\numberline {3.1.5}Multi-layer Perzeptron (Fully Connected Layer)}{13}{subsubsection.3.1.5}
\contentsline {subsection}{\numberline {3.2}Backforward}{13}{subsection.3.2}
\contentsline {subsubsection}{\numberline {3.2.1}Fehlerfunktion}{13}{subsubsection.3.2.1}
\contentsline {subsubsection}{\numberline {3.2.2}Gradient}{14}{subsubsection.3.2.2}
\contentsline {subsubsection}{\numberline {3.2.3}Lernrate}{14}{subsubsection.3.2.3}
\contentsline {subsubsection}{\numberline {3.2.4}Gradientenabstiegsverfahren}{15}{subsubsection.3.2.4}
\contentsline {paragraph}{\numberline {3.2.4.1}Ablauf eines Gradientenverfahrens im \ac {DNN}}{15}{paragraph.3.2.4.1}
\contentsline {paragraph}{\numberline {3.2.4.2}Variante des Gradientenverfahrens}{15}{paragraph.3.2.4.2}
\contentsline {subparagraph}{\nonumberline Stochastic Gradient Descent (SGD):}{16}{section*.23}
\contentsline {subparagraph}{\nonumberline Batch Gradient Descent (BGD):}{16}{section*.24}
\contentsline {subparagraph}{\nonumberline Mini-batch Stochastic Gradient Descent(MSGD):}{17}{section*.25}
\contentsline {section}{\numberline {4}Kompression von \ac {DNN}}{17}{section.4}
\contentsline {subsection}{\numberline {4.1}Pruning Network}{18}{subsection.4.1}
\contentsline {subsection}{\numberline {4.2}Quantisierung von \ac {NN}}{18}{subsection.4.2}
\contentsline {subsection}{\numberline {4.3}Huffman Codierung}{20}{subsection.4.3}
\contentsline {section}{\numberline {5}Overfitting in Convolutional neuronale Netzwerke}{20}{section.5}
\contentsline {subsection}{\numberline {5.1}Overfitting Definition}{20}{subsection.5.1}
\contentsline {subsection}{\numberline {5.2}Strategie gegen Overfitting}{21}{subsection.5.2}
\contentsline {subsubsection}{\numberline {5.2.1}Data Augmentation}{21}{subsubsection.5.2.1}
\contentsline {subsubsection}{\numberline {5.2.2}Dropout}{22}{subsubsection.5.2.2}
\contentsline {paragraph}{\numberline {5.2.2.1}Funktionsweise von Dropout}{23}{paragraph.5.2.2.1}
\contentsline {paragraph}{\numberline {5.2.2.2}Verhinderung der Koadaptationen zwischen Neuronen}{23}{paragraph.5.2.2.2}
\contentsline {paragraph}{\numberline {5.2.2.3}Automatische Erh\IeC {\"o}hung von Training Data und Regelung}{24}{paragraph.5.2.2.3}
\contentsline {subsubsection}{\numberline {5.2.3}Batch-Normalisierung}{24}{subsubsection.5.2.3}
\contentsline {section}{\numberline {6}Experiment}{26}{section.6}
\contentsline {subsection}{\numberline {6.1}Besondere \acsp {CNN}}{26}{subsection.6.1}
\contentsline {subsubsection}{\numberline {6.1.1}AlexNet}{26}{subsubsection.6.1.1}
\contentsline {subsubsection}{\numberline {6.1.2}SqueezeNet}{27}{subsubsection.6.1.2}
\contentsline {subsubsection}{\numberline {6.1.3}Xception: Extreme Inception}{28}{subsubsection.6.1.3}
\contentsline {subsubsection}{\numberline {6.1.4}MobileNet}{29}{subsubsection.6.1.4}
\contentsline {subsection}{\numberline {6.2}Lernrate}{29}{subsection.6.2}
\contentsline {subsection}{\numberline {6.3}Algorithmen zur Optimierung des Gradientenabstiegsverfahren: Optimizer}{31}{subsection.6.3}
\contentsline {subsubsection}{\numberline {6.3.1}Adaptive Gradient Algorithm (AdaGrad)}{31}{subsubsection.6.3.1}
\contentsline {subsubsection}{\numberline {6.3.2}Root Mean Square Propagation(RMSProp)}{31}{subsubsection.6.3.2}
\contentsline {subsubsection}{\numberline {6.3.3}Adaptive Moment Estimation(Adam)}{32}{subsubsection.6.3.3}
\contentsline {section}{\numberline {7}Abk\IeC {\"u}rzungsverzeichnis}{32}{section.7}
